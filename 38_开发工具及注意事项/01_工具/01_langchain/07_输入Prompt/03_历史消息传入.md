这是一个相对老一些的langchain用法，使用时会报警告，但是还能用，可以考虑使用新的方法替换。

```python
from typing import List, Dict

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import SystemMessage, AIMessage, HumanMessage
from langchain_core.runnables import RunnableParallel

history: List[Dict[str, str]] = None
# 将对话转换为 AIMessage 和 HumanMessage 对象
history_messages = [
    HumanMessage(content=msg["content"]) if msg["role"] == "user"
    else AIMessage(content=msg["content"])
    for msg in history
]

SYS_MSG_RAG = """###参考信息###
{doc}

###任务###
你是一个RAG问答机器人，请基于参考信息问答。具体要求如下："""

# 其中format用于替换提示词中带花括号的内容，传入变量
if history:
    # 定义 ChatPromptTemplate，加入系统消息
    template = ChatPromptTemplate.from_messages([
        SystemMessage(content=SYS_MSG_RAG.format(doc=docs)),
        MessagesPlaceholder(variable_name="history"),
        HumanMessage(content=USER_MSG.format(query=query))
    ])
else:
    template = ChatPromptTemplate.from_messages([
        SystemMessage(content=SYS_MSG_RAG.format(doc=docs)),
        HumanMessage(content=USER_MSG_RAG.format(query=query))
    ])

 # 使用 .format_prompt 动态传入历史对话
filled_prompt = template.format_prompt(
    history=history_messages) if history else template.format_prompt()

# 输出结果（用于传入LLM）
llm = ChatOpenAI(
            model=llm_config['llm_mdl_name'],
            openai_api_key=llm_config['llm_api_key'],
            openai_api_base=llm_config['llm_server_url'],
            max_tokens=llm_config['llm_max_tokens'],
            temperature=llm_config['llm_temperature']
        )
# print(filled_prompt.messages)
response = llm(messages=filled_prompt.messages)
ret = response.content
logger.info(f'Thinking-Response: {ret}')
```