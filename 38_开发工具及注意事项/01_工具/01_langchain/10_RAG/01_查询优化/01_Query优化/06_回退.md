参考：https://www.langchain.com.cn/use_cases/query_analysis/techniques/step_back

后退提示

有时候，在处理特定问题时，搜索质量和模型生成可能会出现问题。处理这个问题的一种方法是首先生成一个更抽象的“后退”问题，并基于原始问题和后退问题进行查询。

例如，如果我们提出一个形如“为什么我的LangGraph代理astream_events返回{LONG_TRACE}而不是{DESIRED_OUTPUT}”的问题，如果我们使用更通用的问题“LangGraph代理如何使用astream_events？”搜索，我们很可能会检索到更相关的文档，而不是使用具体的用户问题进行搜索。

让我们看看如何在LangChain YouTube视频的问答机器人中使用后退提示。

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
 
system = """你是一个专家，擅长将特定问题转化为更通用的问题，从而找到回答特定问题所需的基本原则。
 
您将被问及一组用于构建以LLM为基础的应用程序的软件，包括LangChain、LangGraph、LangServe和LangSmith。
 
LangChain 是一个Python框架，提供了一组大型的集成，可以轻松组合构建LLM应用程序。
LangGraph 是一个基于 LangChain 的Python包，它使构建有状态的多角色LLM应用程序变得容易。
LangServe 是一个基于 LangChain 的Python包，它使将 LangChain 应用程序部署为 REST API 变得容易。
LangSmith 是一个平台，它使追踪和测试LLM应用程序变得容易。
 
针对这些产品中的一个或多个，给出一个特定用户问题的更通用问题，以便回答特定问题。
 
如果您不认识某个单词或首字母缩略词，请不要尝试重新编写它。
 
请写出简明扼要的问题。"""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
step_back = prompt | llm | StrOutputParser()
```