# 1. 简介
参考：https://www.langchain.com.cn/modules/data_connection/retrievers/contextual_compression

检索中的一个挑战通常是您在将数据导入系统时不知道文档存储系统将面临的具体查询。这意味着与查询相关的信息可能深藏在包含大量无关文本的文档中。将完整文档通过应用程序传递可能会导致更昂贵的LLM调用和更差的响应。

上下文压缩旨在解决此问题。思路很简单：不是立即返回检索到的文档，而是使用给定查询的上下文对其进行压缩，以便只返回相关信息。这里的“压缩”既指单个文档内容的压缩，也指整个文档的过滤。

要使用上下文压缩检索器，您需要：

- 一个基本检索器
- 一个文档压缩器

上下文压缩检索器将查询传递给基本检索器，获取初始文档并将其通过文档压缩器传递。文档压缩器接受文档列表，通过减少文档内容或完全删除文档来缩短列表。


# 2. 使用
## 2.1 基本用法
现在让我们用ContextualCompressionRetriever包装我们的基本检索器。我们添加一个LLMChainExtractor，它将迭代最初返回的文档，并从每个文档中提取与查询相关的内容。

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import OpenAI
 
def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


llm = OpenAI(temperature=0)
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)
 
compressed_docs = compression_retriever.get_relevant_documents(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

## 2.2 LLMChainFilter

LLMChainFilter是一个略微简单但更强大的压缩器，它使用LLM链来决定最初检索到的文档中应过滤掉哪些文档，哪些文档应返回，而不更改文档内容。

```python
from langchain.retrievers.document_compressors import LLMChainFilter
 
_filter = LLMChainFilter.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=_filter, base_retriever=retriever
)
 
compressed_docs = compression_retriever.get_relevant_documents(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

## 2.3 EmbeddingsFilter

在每个检索到的文档上进行额外的LLM调用是昂贵且慢的。EmbeddingsFilter提供了一种更便宜更快的选项，它通过嵌入文档和查询，并仅返回那些嵌入与查询足够相似的文档。

```python
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_openai import OpenAIEmbeddings
 
embeddings = OpenAIEmbeddings()
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter, base_retriever=retriever
)
 
compressed_docs = compression_retriever.get_relevant_documents(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

## 2.4 将多个压缩器依次组合在一起

使用DocumentCompressorPipeline，我们还可以轻松地将多个压缩器依次组合在一起。除了压缩器之外，我们还可以向管线中添加BaseDocumentTransformer，这些转换器不执行任何上下文压缩，而只是对一组文档进行一些转换。例如，TextSplitter可以用作文档转换器，将文档拆分为较小的片段，而EmbeddingsRedundantFilter可以用于基于文档之间的嵌入相似性来过滤冗余文档。

在下面的示例中，我们首先将文档拆分为较小的块，然后删除冗余文档，然后根据与查询相关性进行过滤。

```python
from langchain.retrievers.document_compressors import DocumentCompressorPipeline
from langchain_community.document_transformers import EmbeddingsRedundantFilter
from langchain_text_splitters import CharacterTextSplitter
 
splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=". ")
redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)
relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
pipeline_compressor = DocumentCompressorPipeline(
    transformers=[splitter, redundant_filter, relevant_filter]
)
```

```python
compression_retriever = ContextualCompressionRetriever(
    base_compressor=pipeline_compressor, base_retriever=retriever
)
 
compressed_docs = compression_retriever.get_relevant_documents(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```