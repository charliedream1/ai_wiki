# 原理

```text
算法 1：使用优化器 M 优化 Φ

输入:
1. 优化器 M，初始程序 Φ，度量 μ
2. 最大迭代次数 I，训练数据 D
3. 小批量大小 B，提议者超参数 θ

输出: 优化后的 Φ

步骤:

1. M.Initialize(D, θ) ⟵ 使用数据初始化优化器
2. 对于 k 从 1 到 I，重复以下步骤：
    1. (V ⟶ S_k) ⟵ M.Propose(θ) ⟵ 生成提案
    2. D_k ⟵ {(x_j, x'_j) ∼ D}^B_{j=1} ⟵ 采样大小为 B 的批次
    3. σ ⟵ 1/B ∑_{(x, x') ∈ D_k} μ(Φ_{V ⟶ S_k}(x), x') ⟵ 验证更新后的程序
    4. M.Update(V ⟶ S_k, σ) ⟵ 基于观察到的验证得分更新优化器
3. (V ⟶ S) ⟵ M.ExtractOptimizedSets()
4. 返回 Φ_{V ⟶ S}
```

- 教训一：自动生成few-shot示例是性能提升的利器
- 教训二：使用MIPRO优化指令和少量示例会产生最佳的整体性能
- 教训三：指令优化对复杂任务而言至关重要
- 教训四：grounding要因任务施策，切忌生搬硬套
- 教训五：优化器的性能取决于优化预算

# 参考

[1] DSPy程序优化Prompt的五个重要教训，NLP 的未来不是修改Prompt，而是构建更高抽象级别的模块，https://mp.weixin.qq.com/s/TF6YAUCga5fNbgIzAEvSRg