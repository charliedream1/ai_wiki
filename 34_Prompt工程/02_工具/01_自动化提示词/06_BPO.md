https://github.com/thu-coai/BPO

大型语言模型 （LLM） 在各种应用中取得了令人印象深刻的成功。然而，这些模型通常与人类意图不太一致，这需要对它们进行额外的处理;也就是说，对齐问题。为了使 LLM 更好地遵循用户指令，现有的对齐方法主要侧重于进一步训练它们。然而，就 GPU 计算而言，LLM 的额外训练通常很昂贵;更糟糕的是，某些 LLM 无法用于用户要求的训练，例如 GPT。在这项工作中，我们采用不同的视角——黑盒提示优化 （BPO）——来执行对齐。这个想法是优化用户提示以适应 LLM 的输入理解，以便在不更新 LLM 参数的情况下最好地实现用户的意图。BPO 利用人类偏好来优化提示，从而使其作为提示工程师优于 LLM（例如 ChatGPT）。此外，BPO 与模型无关，实证结果表明，与 BPO 对齐的 ChatGPT 的胜率比其原始版本提高了 22%，GPT-4 的胜率提高了 10%。值得注意的是，BPO 对齐的 LLM 可以优于 PPO 和 DPO 对齐的相同模型，并且在将 BPO 与 PPO 或 DPO 相结合时，它还带来了额外的性能提升。代码和数据集将于 https://github.com/thu-coai/BPO 发布。
