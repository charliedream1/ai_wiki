# 声明

其中，回答部分来自GPT4o, 人工做了审核，仅供参考。

### **传统的相对位置编码原理**

传统的**相对位置编码**是为了解决**绝对位置编码**的一些缺陷，特别是它无法很好地捕捉**序列中位置之间的相对关系**。

---

### **背景：绝对位置编码的不足**  
- 在**绝对位置编码**中，每个位置（例如位置1、位置2等）都有一个唯一的编码。  
- 这种方法的问题在于：**模型只能知道位置的绝对值，而不知道两个位置之间的相对关系**（例如 “我与左边的词相距多少”）。  
- 这对捕捉**序列中词与词之间的相对位置关系**不利，特别是在长序列中。

---

### **相对位置编码的核心思想**  
- **关键点：** 相对位置编码关注的不是**当前位置的绝对值**，而是**两个词之间的位置差**。  
- 比如：  
   - 在一个句子里，**词A** 和 **词B** 的相对位置可以是 “词A 比词B 在左边 2个位置”。  
   - 模型会编码这种相对位置信息，而不是词A和词B的绝对位置。  

---

### **具体实现**  
传统的相对位置编码主要作用在**注意力机制**的计算中。

1. **相对位置差值**  
   - 首先计算出两个词之间的相对位置差，记为 \((i - j)\)，其中 \(i\) 是查询位置，\(j\) 是键的位置。

2. **引入位置偏置（bias）**  
   - 在注意力得分的计算公式中，加入一个额外的偏置项 \(b_{ij}\)：  
     \[
     \text{Attention}_{ij} = Q_i \cdot K_j + b_{ij}
     \]  
     其中：  
     - \(Q_i \cdot K_j\) 是标准的注意力分数，表示查询和键之间的相似度。  
     - \(b_{ij}\) 是与位置差 \((i - j)\) 相关的偏置，体现了**位置之间的相对关系**。

3. **偏置的设计**  
   - \(b_{ij}\) 的值可以通过预定义的**相对位置编码表**来获取，比如通过一个固定的函数或训练得到的参数来表示不同位置差的权重。

---

### **通俗的例子**  
假设我们有一个句子：**“猫 在 桌子 上 睡觉”**。  

- 绝对位置编码告诉模型：  
   - “猫” 是位置1  
   - “桌子” 是位置3  

- 但这没有告诉模型“猫”和“桌子”之间的位置关系。  
- **相对位置编码**则告诉模型：  
   - “猫” 和 “桌子” 的相对位置是 **-2**（向左移动2个位置）。  
   - 这让模型理解“猫” 和 “桌子”的关系更加清晰，而不是依赖绝对位置。  

---

### **优点**  
1. **更强的泛化能力**  
   - 相对位置编码能让模型更好地理解**词与词之间的关系**，即使句子长度变化，位置关系仍然有效。

2. **适用于长序列**  
   - 长序列中，即使词的位置发生偏移，词之间的相对关系依然是稳定的。

3. **更自然地捕获关系**  
   - 语言本质上是关于词之间的关系，相对位置编码更接近语言的语义结构。

---

### **总结**  
- 传统的相对位置编码将关注点从“绝对位置”转移到了“位置之间的相对差异”。  
- 通过将**相对位置偏置**加入注意力机制，模型能捕获词与词之间的位置关系，提升理解能力。  
- 相对位置编码使模型对**序列长度变化**具有更好的适应能力。
