# 1. 方式1

相对最直白简单，和原始的openai传入的messages很类似

```python
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage

# 模型用ollama在本地启动的
llm_model = ChatOllama(model=llm_config['mdl_name'],
                               temperature=llm_config['llm_temperature'])
# 其中research_topic和language是提示词的变量
reflection_instructions = """You are an expert research assistant analyzing a summary about {research_topic}.

<GOAL>
1. Identify knowledge gaps or areas that need deeper exploration
2. Generate a follow-up question that would help expand your understanding
3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered
</GOAL>

<REQUIREMENTS>
Ensure the follow-up question is self-contained and includes necessary context for web search. 
Please give out answer with language: {language}.
</REQUIREMENTS>"""
# 直接调用模型
result = llm_model.invoke(
        [SystemMessage(content=reflection_instructions.format(research_topic=state.research_topic,
                                                              language=configurable.language)),
        HumanMessage(content=f"Identify a knowledge gap and generate a follow-up web search query based on our existing knowledge: {state.running_summary}")]
    )   
```