大模型定义：

```python
from langchain_openai import ChatOpenAI
llm_model = ChatOpenAI(
            model=llm_config['llm_mdl_name'],
            openai_api_key=llm_config['llm_api_key'],
            openai_api_base=llm_config['llm_server_url'],
            # NOTICE: latest version of langchain doesn't support this parameter
            #  it might be problem of either langchain or openai lib
            # max_tokens=llm_config['llm_max_tokens'],
            temperature=llm_config['llm_temperature'],
        )
```

比如，图中的一个节点功能如下：

```python
# Nodes   
def generate_query(state: SummaryState, config: RunnableConfig):
    """ Generate a query for web search """
    
    # Format the prompt
    configurable = Configuration.from_runnable_config(config)
    query_writer_instructions_formatted = query_writer_instructions.format(research_topic=state.research_topic,
                                                                           language=configurable.language)
    # Generate a query
    result = llm_model.invoke(
        [SystemMessage(content=query_writer_instructions_formatted),
        HumanMessage(content=f"Generate a query for web search:")]
    )   
    query = json_repair.loads(result.content)
    
    return {"search_query": query['query']}
```

如果大模型不需要流式输出，仅流式显示模型运行到哪一个节点了，可以用下面方法：

```python
ret = ''        
for output in graph.stream(inputs):
    # key是图节点名称，value是节点返回的结果
    for key, value in output.items():
        # Node
        pprint(f"运行完成：{key}:")
        # Optional: print full state at each node
        pprint(value, indent=2, width=80, depth=None)
        if key == 'summarize_sources':
            for item in value:
                print(item)
        ret = value.get("running_summary", '')
pprint(ret)
ret = ret.lstrip('## Summary\n\n').split('### Sources:')[0].strip()
return ret
```

如果需要节点流式显示，同时大模型也流式输出，代码如下：

stream 方法还有一个 stream_mode 参数，该参数有以下几个值可选

'values': Emit all values of the state for each step.
'updates': Emit only the node name(s) and updates
that were returned by the node(s) after each step.- 'debug': Emit debug events for each step.
'messages': Emit LLM messages token-by-token.
'custom': Emit custom output write: StreamWriter kwarg of each node.

注意，大模型的定义部分llm_model的stream参数不要传True，否则会报错，因为graph会自动传入这个参数

```python
inputs = {"messages": [HumanMessage(content="你好")]}
ret = ''
for output in graph.stream(inputs, stream_mode=["updates", "messages"]):
    if output[0] == "messages":
        # 大模型返回，图返回结果
        message_data, graph_data = output[1]
        # print(output[0], {graph_data.get("langgraph_node"): message_data.to_json()})
        # 大模型流式输出
        print(output[0], {graph_data.get("langgraph_node"): message_data.content})
    else:
        print('运行完成：', output[0], output[1])
        print('**' * 20)
        if 'finalize_summary' in output[1]:
            # 为了取出最终的结果
            ret = output[1]['finalize_summary']['running_summary']

pprint(ret)
```

# 参考

[1] (注意这个参考写的挺全面，但里面有不少细节错误，仅供参考), https://www.yangyanxing.com/llmdev/langgraph/index.html
