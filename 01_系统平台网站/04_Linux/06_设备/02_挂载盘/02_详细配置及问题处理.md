# 1. 免密登录设置

请参考之前的第一篇，保证所有集群里的服务器能互相免密登录。

# 2. 使用共享盘方案（首推方案）

本方法相对最省事方便

## 2.1 配置共享挂载盘

1. 安装NFS服务
    
    ```bash
    sudo apt-get install nfs-kernel-server
    ```

2. 在服务器上创建共享目录，并设置权限。

    ```bash
    mkdir /mnt
    chmod -R +777 /mnt
    ```

3. 配置NFS配置文件

    ```bash
    vim /etc/exports
    ```

    添加如下内容 (配置给哪些服务器共享)

    ```text
    /mnt 192.168.16.11/22(insecure,rw,sync,no_root_squash,no_subtree_check) 192.168.16.12/22(insecure,rw,sync,no_root_squash,no_subtree_check)
    ```
   
    这行代码的意思是把共享目录/mnt共享给192.168.16.11和192.168.16.12
    
    其中：
    - rw 表示设置目录可读写。
    - sync 表示数据会同步写入到内存和硬盘中，相反 rsync 表示数据会先暂存于内存中，而非直接写入到硬盘中。
    - no_root_squash NFS客户端连接服务端时如果使用的是root的话，那么对服务端分享的目录来说，也拥有root权限。
    - no_all_squash 不论NFS客户端连接服务端时使用什么用户，对服务端分享的目录来说都不会拥有匿名用户权限。
    - 如果有多个共享目录配置，则使用多行，一行一个配置。
    - 保存好配置文件后，需要执行以下命令使配置立即生效：
      ```bash
      exportfs -r
      exportfs -a
      systemctl enable nfs-kernel-server && systemctl restart nfs-kernel-server
      ```

      查看该服务器（主机）可挂载的目录
      ```bash
      showmount -e
      ```

4. 启动服务-服务端
 
   按顺序启动rpcbind和nfs服务：
    ```bash
    systemctl start rpcbind
    service nfs-server start 
    ```

   加入开机启动：
   ```bash
    systemctl enable rpcbind 
    systemctl enable nfs
   ```
  
   nfs服务启动后，可以使用命令 rpcinfo -p 查看端口是否生效。
    然后使用 showmount 命令来查看服务端(本机)是否可连接：
    ```bash
    [root@localhost ~]# showmount -e localhost
    Export list for localhost:
    /data/share 192.168.16.11
    ```
   
    出现上面结果表明NFS服务端配置正常。
    客户端配置

5. 终端-挂载远程nfs文件系统

    查看服务端已共享的目录 （192.168.16.1使用本机的地址）:
    ```bash
     [root@localhost ~]# showmount -e 192.168.16.1
     Export list for 192.168.16.11:
     /data/share 192.168.16.12
    ```

    建立挂载目录，执行挂载命令：
    ```bash
    mkdir -p /data/share
    ```

    终端执行
    ```bash
    mount  -t nfs -o rw,bg,soft,nolock 192.168.16.1:/data/share /data/share
    ```

    如果不加 -onolock,nfsvers=3 则在挂载目录下的文件属主和组都是nobody，如果指定nfsvers=3则显示root。
  
    如果要解除挂载，可执行命令：
   
    ```bash
    umount -f -l /data/nvme3/
    ```

    - -f  强制umount，即使nfs server仍然存在
    - -l  懒umount，文件系统不繁忙的时候移除所有引用

6. 开机自动挂载（可不配）

   如果按本文上面的部分配置好，NFS即部署好了，但是如果你重启客户端系统，发现不能随机器一起挂载，需要再次手动操作挂载，这样操作比较麻烦，因此我们需要设置开机自动挂载。我们不要把挂载项写到/etc/fstab文件中，因为开机时先挂载本机磁盘再启动网络，而NFS是需要网络启动后才能挂载的，所以我们把挂载命令写入到/etc/rc.d/rc.local文件中即可。

   ```bash
   [root@localhost ~]# vim /etc/rc.d/rc.local
   ```
   
   在文件最后添加一行：
   ```bash
   mount  -t nfs -o rw,bg,soft,nolock 192.168.16.1:/data/share /data/share/  
   ```

   如果出现 Stale file handle

   ```bash
   umount -f -l /data/nvme3/
   ```

   如果出现:
   ```bash
   mount.nfs: backgrounding "192.168.16.1:/data/nvme3"
   mount.nfs: mount options: "rw,bg,soft,nolock"
   ```

   需要远重新加载服务端 （该问题多由于服务端重启了）

   如果卸载盘再重新挂载还是不行，可以在服务端执行以下命令：

   ```bash
   sudo systemctl restart nfs-server
   ```
  
   如果还是不行，可以进一步在客户端执行以下命令：

   ```bash
   sudo systemctl restart nfs-client.target
   ```
  
7. 测试验证（可不配）

    查看挂载结果，在客户端输入 df -h
    
    ```bash
    文件系统    容量 已用 可用 已用% 挂载点
    192.168.16.1: /data/share/   xG xG xG x% /data/share/ 
    ```
   
    最后一行说明已经挂载成功了。接下来就可以在客户端上进入目录/data/share/ 下，新建/删除文件，然后在服务端的目录/data/share查看是不是有效果了，同样反过来在服务端操作在客户端对应的目录下看效果。

## 2.2 共享Conda环境

***重要：conda环境需要放开所有权限***

```bash
chmod -R +777 /root/miniconda3h
```

1. 在所有服务器的相同home目录下创建一个conda环境
2. 在主服务器（其中一台，可以选择共享磁盘的那台）创建一个conda环境
3. 将conda的envs目录下的环境文件夹整个挪到共享盘
4. 在所有服务器的相同home目录下创建一个软链接，指向共享盘的conda环境
    ```
    ln -s /mnt/conda_envs ~/miniconda3/envs
    ```
5. 更新conda包时，可直接在主服务器上更新，其他服务器会自动同步

## 2.3 共享训练脚本

- 训练脚本放在共享盘上，所有服务器都可以访问，需要修改目录权限
- 模型存放输出目录也需要设置权限

```bash
chmod -R +777 /mnt/train_script
```

### 2.3.1 cache配置及问题

```bash
chmod -R +777 /mnt/train_script
```

***坑点***
启动时某一台服务器有50%的概率可能会报如下错误，导致训练无法启动：

```text
FileNotFoundError: [Errno 2] No such file or directory: '.incomplete/dataset_info.json
```

原因：

如果制定了数据的cache目录是共享盘的同一个路径位置，加载数据时，所有服务器都会往里写数据，由于写入的时间差，
导致个别文件不一致，从而导致某一台服务器挂掉 (实测有50%的概率出现该问题)。

解决方法：

不要指定到同一个目录，删除改配置，代码会自动使用/home目录，或者指定各自服务器路径相同但不共享的一个路径。
对应参数：--cache_dir ${output_dir}_cache

### 2.3.2 hostfile

创建一个文件hostfile，存储使用哪些机器，第一个和启动脚本的机器需要是同一个。slots8代表使用所有8个卡。huaweiyun1是免密登陆中设置的机器别名

```text
huaweiyun1 slots=8
huaweiyun2 slots=8
```

### 2.3.3 deepspeed启动

```shell
deepspeed  --hostfile hostfile \
    src/train.py \
    --deepspeed ${ds_config}
```

# 3. 无共享盘方案

如果共享盘大小不够，或者网络通讯速度不够，可以使用无共享盘方案。即所有的conda环境、训练脚本及数据文件、输出目录均在各自的机器上，但需要
保持所有的路径是相同的 （只有这样，才能共享一套同样的脚本执行）。

注意：

```bash
chmod -R +777 /root/miniconda3h
```

- conda环境需要放开所有权限
- 训练脚本需要全部放开所有权限
- 数据目录需要全部放开所有权限

## 3.1 启动脚本

启动脚本如下：

```shell
#! /usr/bin/env bash

set -ex
#export NCCL_NET=IB
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=ibs11,ibs12
export NCCL_IB_HCA=mlx5_0:1,mlx5_1:1

prj_path=/home/llm_trainer  
DATESTR=`date +%Y%m%d-%H%M%S`
RUN_NAME=chatglm3
OUTPUT_DIR=/data/nvme0/output/chatglm3/${RUN_NAME}-${DATESTR}
mkdir -p "$OUTPUT_DIR"

# NOTICE:
#  1. you need to change authority of the running shell script to make it work
#  2. you need to change the path of the running shell script to make it work
#  3. you need to synchronize program files, data files and model files to make it work
#  4. num_nodes should be same with the number in hostfile
#  5. output will be saved on the 1st machine in hostfile since no nomination at here
deepspeed --no_python --hostfile=hostfile --num_nodes=2 --num_gpus 8 \
          ${prj_path}/egs_mdl/sft/v1_multi_nodes/run.sh \
	  1>${OUTPUT_DIR}/train.log \
	  2>${OUTPUT_DIR}/train.err
```

如上命令启动2个节点，每个节点8个卡，共16个卡。

上述脚本中run.sh脚本(主要控制训练配置)如下：

```shell
/home/miniconda3/condabin/conda run -n llm_train \
  python src/train_bash.py \
    --deepspeed ${ds_config}
```
