Rank1: Test-Time Compute for Reranking in Information Retrieval（生成式reranker微调）
借助推理语言模型推理能力，微调提升生成式reranker模型的性能。

数据准备

数据来源：使用MS MARCO数据集，通过R1模型，生成数据。最终数据集包含635,264个R1生成的样本，其中62.9%被标记为相关，37.1%被标记为不相关。推理链的长度分布接近正态分布，相关和不相关样本的分布没有明显差异。

数据混合：尝试了多种方法来确定最终的数据混合。最初尝试使用所有数据，但在平衡标签后发现性能显著低于仅使用最确定的标签（即MS MARCO的正样本和Tevatron的负样本）的模型。

质量过滤：使用在第一种混合数据上训练的模型进行自我过滤，过滤掉了大约10%的数据，主要是假负样本。由于仍然有足够的“相关”标记实例，最终训练集包括386,336个高质量训练样本：136k来自原始MS MARCO正样本，154k来自Tevatron负样本，96k来自mT5负样本。

模型训练

选择了Qwen 2.5进行训练，通过LLaMA-Factory使用LoRA微调，发现在大约1.5个周期后性能开始饱和。

# 参考

[1] https://mp.weixin.qq.com/s/Lvq0vH2GpvVq_zsSCo7S9w