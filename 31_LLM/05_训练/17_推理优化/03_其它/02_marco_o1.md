# 1. èµ„æº

- é˜¿é‡Œå›½é™…å¼€æº
- modelscopeæ¨¡å‹ä¸‹è½½ï¼šhttps://modelscope.cn/models/AIDC-AI/Marco-o1
- Github (938 star): https://github.com/AIDC-AI/Marco-o1
  - é‡Œé¢æ²¡æœ‰ä»€ä¹ˆæœ‰ä»·å€¼çš„ä¸œè¥¿ï¼Œæ‰€è°“çš„Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategieså‡æ²¡æœ‰
- è®ºæ–‡ï¼šMarco-o1: Towards Open Reasoning Models for Open-Ended Solutions
  - https://arxiv.org/abs/2411.14405

ç®€è¦è¯„ä»·ï¼š
- ä»…é€šè¿‡é•¿æ¨ç†é“¾COTæ•°æ®è¿›è¡ŒSFTè®­ç»ƒï¼Œæ²¡æœ‰æ‰€è°“çš„ç±»ä¼¼STEP-DPOçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
- æ¨ç†æµç¨‹ä¸­çš„MCTSä¹‹ç±»æ–¹æ³•å¹¶æœªå¼€æº

Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and codingâ€”which are well-suited for reinforcement learning (RL)â€”but also places greater emphasis on open-ended resolutions. 

Currently, Marco-o1 Large Language Model (LLM) is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategiesâ€”optimized for complex real-world problem-solving tasks.

# 2. åŸç†

MCTSæ–¹æ³•

![](.02_marco_o1_images/MCTSæ–¹æ³•.png)

Currently, our work is distinguished by the following highlights:

ğŸ€ Fine-Tuning with CoT Data: We develop Marco-o1-CoT by performing full-parameter fine-tuning on the base model using open-source CoT dataset combined with our self-developed synthetic data.

ğŸ€ Solution Space Expansion via MCTS: We integrate LLMs with MCTS (Marco-o1-MCTS), using the model's output confidence to guide the search and expand the solution space.

ğŸ€ Reasoning Action Strategy: We implement novel reasoning action strategies and a reflection mechanism (Marco-o1-MCTS Mini-Step), including exploring different action granularities within the MCTS framework and prompting the model to self-reflect, thereby significantly enhancing the model's ability to solve complex problems.

ğŸ€ Application in Translation Tasks: We are the first to apply Large Reasoning Models (LRM) to Machine Translation task, exploring inference time scaling laws in the multilingual and translation domain.

Marco-o1 leverages advanced techniques like CoT fine-tuning, MCTS, and Reasoning Action Strategies to enhance its reasoning power. As shown in Figure 2, by fine-tuning Qwen2-7B-Instruct with a combination of the filtered Open-O1 CoT dataset, Marco-o1 CoT dataset, and Marco-o1 Instruction dataset, Marco-o1 improved its handling of complex tasks. MCTS allows exploration of multiple reasoning paths using confidence scores derived from softmax-applied log probabilities of the top-k alternative tokens, guiding the model to optimal solutions. Moreover, our reasoning action strategy involves varying the granularity of actions within steps and mini-steps to optimize search efficiency and accuracy.

![](.02_marco_o1_images/æ¨ç†æµç¨‹.png)

ğŸŒ As shown in Figure 3, Marco-o1 achieved accuracy improvements of +6.17% on the MGSM (English) dataset and +5.60% on the MGSM (Chinese) dataset, showcasing enhanced reasoning capabilities.

![](.02_marco_o1_images/æ€§èƒ½.png)

ğŸŒ Additionally, in translation tasks, we demonstrate that Marco-o1 excels in translating slang expressions, such as translating "è¿™ä¸ªé‹æ‹¥æœ‰è¸©å±æ„Ÿ" (literal translation: "This shoe offers a stepping-on-poop sensation.") to "This shoe has a comfortable sole," demonstrating its superior grasp of colloquial nuances.

![](.02_marco_o1_images/ç¿»è¯‘æ¨ç†.png)

