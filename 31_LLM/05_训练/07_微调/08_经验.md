**SFT 和 in-context learning 的区别是什么？**

In-context learning 目的是通过一个或者几个例子激发大模型对任务的能力（也就是 prompt）。

In-context learning 在 GPT3 论文中提出，并不会修改模型的参数和梯度，不参与反向传播过程。

**如何选择 SFT 的基座模型？**

第一，选择 base 模型还是 chat 模型作为 SFT 基座？

Base 模型提供了基本的语言理解和生成能力，而 Chat 模型通过指令微调和人工反馈强化学习等方法，使模型更加符合人类的价值观和指令要求。

一般情况下选择 chat 模型为微调基座，因为 base 模型还没有指令遵循或对话能力。

第二，基座模型选择大模型还是小模型？例如 Qwen3-235B 或 0.6B。取决于应用场景，对于单一任务来说，小模型已经够用。

第三，modelscope 社区一般会给出 3 种类型的模型，第一种是预训练模型，第二种有 chat 能力的微调模型，第三种是在 chat 能力基础上的量化模型（量化模型通俗来说就是把大模型中某些层参数由 float16 变为 int8 等）以节省显存。

**SFT 的训练数据集如何构建？**

第一，对于不同的微调任务和微调基座模型，生成特定输入格式的数据。

通常，微调任务主要包括根据指令简单对话，根据指令实现上下文对话（更加关注上下文的逻辑与联系）这两种。

目前被大家广泛使用的是两种微调数据格式为 Alpaca（JSON 结构）和 share GPT，前者使用较多，后者更加关注长对话。

微调数据中的 COT 更加适用的场景是-数学、证明等需要逻辑推理的场景，而 COT 数据通常包括 question、COT、Answer 这三个部分。多轮对话通常在最后一轮问答时使用思维链。

第二，微调数据获得方式主要包括人工生成，人工和大模型协助（RAG 或大模型 prompt）生成两种。

摘抄：要想尽各种办法去扩充 prompt 的任务多样性和表达方式多样性，甚至去刻意加一些 noisy prompt 去提升抗噪性。

摘抄：选一个默认的 json 格式：带不带 markdown，indent 设置成几，是否输出在一行，然后把 sft 中所有涉及到的 json 数据全部清洗成这种格式。

第三，获得高质量的微调训练数据是 SFT 的核心。作为 SFT 从业者，日常 95% 以上的时间就是生产数据、分析数据、清洗数据。

摘抄：这段时间天天 sft，怎么整都提不了效果，最后老老实实用规则+人工把数据一条条洗一遍，模型才终于稳定了。特别是复杂推理任务，数据里 answer 有 conflict 的点的话，模型经常学疯。

第四，Instructions 这个一定要给出，让大模型明确自身的定位。input 可以不给，模型也可以直接通过 instructions 给输出。

**SFT 需要多少数据量？**

2K-10W，当然要具体任务具体分析。

摘抄：通常情况下，仅需约“一万份样本”便足以达成理想的微调成果。

这一理念在 Meta 发布的《LIMA: Less Is More for Alignment》论文中得到了有力阐述，该文献强调了在指令微调过程中，高品质微调数据的决定性作用。

据此，我们应当将重心放在提升样本质量的打磨上，而非单纯追求数量的增长。

**如何评价 SFT 训练数据集的质量？**

有多个维度，包括样本多样性、答案质量、回答一致性等多项指标。

**SFT 过程对硬件有什么要求？**

第一，微调大模型需要多少显存？训练过程中的显存取决于：模型参数、梯度、优化器、中间激活变量，主要显存消耗在前三个。

经验来说，显存大概是模型参数量的 12 倍，例如模型是 1B，总共显存占用 12GB。

第二，V100 不支持 flash attention。

第三，V100 不支持 bf16，但支持 GPTQ 模型和 FP8。V100 不支持 fp16，但是可以通过修改 config.json 中的 dtype 将 bf16 变为 float32 或 float16。

第四，对于 LORA 微调而言，train 和 test 代码不同。在 test 时候，因为保存的 checkpoint 只有 linear 层，需要先把模型合并到一起。

第五，一般情况下 GPTQ 模型可以用来微调，但是不能全参数微调，必须和 PEFT 一起使用。

尽量不要使用量化后的模型进行 SFT，因为 LORA 是 float16，而量化后的模型不一定是 float16，这样串联在一起是有问题的。

例如，GPTQ 是 mixed int4/fp16，其中激活函数是 fp16，GPTQ 的模型没法和 LORA 层合并，因为 LORA 层都是 fp16 的。

**SFT 训练过程是什么样？**

第一，从 loss 角度，随着 steps 增加，train loss 先急剧下降，而后平缓；evl loss 先急剧下降，而后上升（过拟合）。

第二，为什么 SFT 通常在 epoch=2 时突然 loss 急剧降低？大模型参数量很大，在第一个 epoch 基本记住了整个训练集，所以在第二个 epoch 会突然下降，出现过拟合现象。

第三，摘抄：训 10 个 epoch，如果某些 case 还学不会，说明模型的能力就是不够。

**SFT 过程如何调参？**

关于 checkpoint 的选择：在欠拟合和过拟合之间找到临界点。

learning rate 一般设置为预训练阶段的 0.1 倍。小模型大学习率，大模型小学习率。降低学习率某种程度上可以缓解过拟合。

epoch 和数据量成反比，数据量越大，epoch 个数越少。较大的 epoch 会造成过拟合，epoch 基本上就是 1～3 个。

batchsize 主要取决于使用的硬件内存。

起始训练适当做点 warmup，几种主流的 lr_scheduler 可以都试一下。

gradient_accumulation_steps -16 / 32 / 64 / 128 等数字都可以尝试下（梯度累加就是，每次获取 1 个 batch 的数据，计算 1 次梯度，梯度不清空，不断累加，累加一定次数后，根据累加的梯度更新网络参数，然后清空梯度，进行下一次循环）。

按需求决定是否使用 dropout。

**SFT 有哪些不良后果？如何避免？**

第一，过拟合现象，模型的特定领域能力增加，通用能力会降低。如何缓解 SFT 后模型通用能力的下降？

使用数据配比（增加一些通用生成的数据），也可以考虑PEFT的方法、学习率调整、正则化技术。

第二，出现大模型幻觉。什么是大模型幻觉？大模型乱说，上下文矛盾，prompt 与事实矛盾，荒谬的回复等。

为什么产生幻觉？数据存在噪声，用于训练的数据单一，过拟合，泛化能力弱

**如何评估 SFT 的推理耗时？**

摘抄：模型的预测时间可以近似理解为：k*x+b，其中 b 是首个 token 的耗时，k 是后续每个 token 的耗时，x 是生成 token 的总数量。

更具体的，b 会是 k 的十几倍或更多，和 prompt 的长度几乎呈正相关。这个耗时的近似估算和 KV_cache 机制有关，不熟悉的可以自行搜索。

这也就是为什么众人都知 cot 效果好，众人又都不使用 cot，因为我们可以几乎下断言“模型的生成速度和生成 token 数量呈正相关”，而 cot 恰恰又引入了大量的生成 token。

**什么是 SFT packing？**

SFT packing 指的是在训练 sft 的过程中，将多个 sft 数据 pack 到一个样本内进行训练的方式，这种方式会加快模型训练速度。

如果不进行 SFT packing，那么对于短文本 sft，需要 padding 到一个 batch 的最长长度，会浪费很多计算 token。

优点：充分利用 GPU 算力。

缺点：不利于短文本和多轮对话，一般情况下不建议使用。

**一句话形容 SFT 的原理？**

预训练是 next token prediction 的自监督学习，SFT 是 next token prediction 的监督学习，二者的反馈粒度都是 token。

SFT 像是在背书，一般不存在学不会，只存在不会泛化。

# 参考

[1] SFT 22条实践经验，干活效率翻倍！https://mp.weixin.qq.com/s/IatfSsVkMmzGeRCZM6LK9Q