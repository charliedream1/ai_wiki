RL 的各种 Trick
继续往下聊，既然 RL 容易训崩溃，那都有哪些技巧可以稳定训练呢，这里简单罗列一下：

entropy collapse：训练的时候加不加 entropy loss，至今仍未达成共识。
CLIP：至少一半的强化工作都围绕 clip 做文章，这些工作分析的非常有道理，实际用起来却乏善可陈。
Token Mask：对高熵 / 低熵 token 做特殊的逻辑，或鼓励某些 token 的学习，或阻止某些 token 的更新，也是重点雕花区域。
Reward Shape：
控制 reward 样本中 0 / 1 的分布在一个区间范围内；
用 pass@K 代替 pass@1 作为优化目标；
用 test case 的通过率作为 reward；
length penalty；
……
训推一致性：当下最热的话题，以 tis、iceppop 最为火热，但可以说和算法没啥关系，全看 infra 功底。
怎么说呢，我个人不喜欢加太多技巧，尤其是 entropy_loss、kl_loss 这种不太知道会对模型产生什么影响的技巧。完全是为了在 RL 的过程中控制模型不去崩溃，让那条训练曲线更好看。大多数实验中，熵炸、grad_norm 炸都属于表象，阻止它不如去分析它，以熵为例：

为什么会熵增？按理说训练是一个确定性增加的过程应该熵减，但是你的模型确实在增，那就说明训练的过程中：高概率 token 常被当作负例，或者是低概率 token 被常当作正例；
为什么会熵减过快？rollout 多样性差呗。是不是调整下 rollout temperature 和 rollout prompt 要比加一个 entropy loss 更合理些。
总结下来，任何技巧的本质，都是在帮助模型寻找一个适合它的训练数据分布。因此，分析 rollout 数据分布的变化，优先级要始终领先于尝试引入某个稳定训练的技巧。这些技巧在稳定某次训练的同时，也会掩盖训练崩溃的原因。但同时，若某个技巧确实有用，也可以反过来推哪种数据分布“有利于/有损于”模型的学习：例如， off_policy 和训推不一致会引起崩溃，是不是在间接说明“一个整体上与模型分布很接近，但却在个别 token 上和模型分布差异很大的样本”可能是一种不太适合模型的数据。

引入训练技巧必然会引起训练数据分布的变化，有些分布的变化是在我们预期之内的，有些分布的变化则是我们预期之外且不知情的。CISPO 的作者就曾分享过：在 off_policy 的时候， 被 clip 掉的 token 是具有多重分布的，概率值与当前模型的分布不一致只是其所具有的一个分布，“概率低但影响long cot涌现”则是这些 token 的另外一个分布。作为训练者，我们往往不能留意到所有分布的变化，从而总结出一些错误的结论。

这里我并不是反对所有技巧，而是认为：在使用技巧的时候，我们需要知道自己设计的新的loss 会让哪种分布的 token 得到促进 / 抑制？如果无法得知，那就别加。

RL 数据
那么话说回来了，RL 到底有没有任何时候都有效且毫无副作用的技巧呢？有的，兄弟，有的。洗数据！训 reward model！

先说数据吧，今年大家普遍进入了 post train 深水区之后(从 math、gsm8k 进阶到 aime、imo)，一个很严重的问题就是：训模型者看不懂数据了，没有办法通过肉眼看解题过程来判断数据质量了。而训模型者日常批量清洗数据的手段，往往都存在一个问题：“无法区分难题和错题”。

难题有什么特点？模型多次采样后，屡屡犯错，偶尔灵机一动，做对了。
错题有什么特点？模型多次采样后，基本都做对，但是因为和 ground_truth 不一致屡屡被判错，偶尔脑子抽风做错了，好巧不巧和 ground_truth 一致了。
不要以为错题的答案是离谱到一眼就能看出来的那种，事实上，错题往往是十分接近 ground_truth 且非常具有迷惑性的。这里我举几个例子：

一张票 2 块钱，9 块钱能买几张票？我们以为错题的答案会是 356 张这种离谱的数字，其实是 4.5 张；
一个一元七次方程，错题的答案给了 3 个实数解，我们 review 的时候，反代入进去发现是对的，留下了这道题目。但在训练的时候，模型拿着 3 个实数解和 4 个复数解，高高兴兴的去找 reward_model 要奖励的时候，反手被打了 0 分，这对模型是多大的心理阴影呀。
……
目前的开源 RL 数据的质量真的是一言难尽。没辙，要么请专业的硕博理科生去标注，要么用启发式的规则去清洗，在不够干净的数据上只能得到错误的实验结论。

再说 reward model，千万不要以为所谓的 rule_based reward model 真的就是靠 rule 来打分的，或者是靠 math_verify 这种规则库 。有很多情况下，靠 rule 几乎无解：

问题是盈利__%？标准答案是 96，而模型输出了“盈利96%”，模型活该拿 0 分吗？
标准答案是 3.14，模型输出了 、圆周率、3.1415926，模型活该拿 0 分吗？
……
reward 要准，我建议使用 generate reward，而且得是能力巨强的那种。这个模型需要读的懂题目要求的输出格式、 ground_truth 的等价变换，以及各种复杂的高阶公式。除了较强的知识能力外，模型还要具备很强的指令遵循能力，否则它容易自己亲自下场解题。


# 参考

[1] 大模型RL各种trick, https://mp.weixin.qq.com/s/V8RO0zKWOFJ5jXDMauWBMA