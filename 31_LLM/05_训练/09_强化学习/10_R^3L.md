论文标题：

R³L : Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification

论文链接：

https://arxiv.org/pdf/2601.03715

项目地址:

https://github.com/shiweijiezero/R3L

强化学习推动了大型语言模型（LLM）在推理与智能体能力方面的进步，然而现有方法在探索（exploration）与利用（exploitation）两方面均面临挑战。在探索方面，现有方法在困难任务上的成功率较低，且从头开始反复执行完整推理轨迹（rollout）的成本高昂；在利用方面，存在信用分配粗糙和训练不稳定问题：轨迹级别的奖励机制会因后续错误而惩罚原本合理的前缀部分；而失败占主导的样本会淹没少数正向信号，导致优化过程缺乏建设性方向。



为此，阿里联合香港科技大学、苏州大学提出强化学习方法 R³L ，该方法引入了语言引导探索、关键点信用分配（Pivotal Credit Assignment）和正向信号增强（Positive Amplification）机制。在智能体任务和复杂推理任务上的实验表明，R³L 相较基线方法取得了 5% 至 52% 的相对性能提升，同时保持了训练的稳定性。

# 参考

https://mp.weixin.qq.com/s/gst1izfRCgvVUaTQp1WQMQ