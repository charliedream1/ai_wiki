# 1. SiliconCloud
**优势**:
- 无需高性能硬件
- 部署和维护简单
- 使用 SiliconCloud 可免费调用 internlm/internlm2_5-7b-chat 模型
- 高并发量，推理速度快

**使用说明**:
- 选择"云端模型"选项
- 选择 "internlm_silicon" 作为模型格式
- 输入 SiliconCloud API Key（需在 https://cloud.siliconflow.cn/ 注册获取）

**重要说明**:
- internlm/internlm2_5-7b-chat 模型在 SiliconCloud 上可以免费调用，但 API Key 仍需妥善保管好。
- MindSearch 项目与 SiliconCloud 并无利益关系，只是使用它能更好地体验 MindSearch 的效果，感谢 SiliconCloud 为开源社区所做的贡献。

# 2. Modelscope

每天2000个请求的使用容量

后续会提供更多模型，目前支持的模型如下：

```text
Qwen/Qwen2.5-Coder-32B-Instruct

Qwen/Qwen2.5-Coder-14B-Instruct

Qwen/Qwen2.5-Coder-7B-Instruct

Qwen/Qwen2.5-72B-Instruct

Qwen/Qwen2.5-32B-Instruct

Qwen/Qwen2.5-14B-Instruct

Qwen/Qwen2.5-7B-Instruct
```

使用方法

```python
from openai import OpenAI

client = OpenAI(
    api_key="MODELSCOPE_SDK_TOKEN", # ModelScope Token
    base_url="https://api-inference.modelscope.cn/v1"
)

response = client.chat.completions.create(
    model="Qwen/Qwen2.5-Coder-32B-Instruct", # ModleScope Model-Id
    messages=[
        {
            'role': 'system',
            'content': 'You are a helpful assistant.'
        },
        {
            'role': 'user',
            'content': '用python写一下快排'
        }
    ],
    stream=True
)

for chunk in response:
    print(chunk.choices[0].delta.content, end='', flush=True)
```

在这个范例里，针对魔搭平台提供的API，适配的有几个地方：

```text
base url 指向魔搭的服务：https://api-inference.modelscope.cn/v1/

api_key 使用魔搭的SDK token

模型名字(model)使用魔搭上开源模型的Model Id，例如Qwen/Qwen2.5-Coder-32B-Instruct
```

这其中，魔搭的SDK Token，可以从您的魔搭账号中获取：

https://modelscope.cn/my/myaccesstoken

![](.01_免费大模型_images/SDK获取.png)

即开即用Notebook分享链接：

https://modelscope.cn/notebook/share/ipynb/86e5ec04/Qwen-Coder-inference-api.ipynb.ipynb

# 3. Free QWQ-32B

实测：会报502错误，可能是因为访问量过大，导致服务器压力过大。

Free QWQ-32B。这是世界上第一个完全免费、无限制、无需注册登录的分布式 AI 算力平台，基于 QwQ 32B 大语言模型提供强大的 AI 服务。QwQ 32B是阿里最新的推理型模型，性能和DeepSeek671B-R1满血版持平。

传送门：https://qwq.aigpu.cn/

# 参考

[1] 开发者福利，魔搭推出免费模型推理API，注册就送每日2000次调用！https://mp.weixin.qq.com/s/sjuMbEfOT0dF4pKpCVALeg
