问题：

```text
Token indices sequence length is longer than the specified maximum sequence length for this model (8966 > 8192). Running this sequence through the model will result in indexing errors
```

出现如上问题，可能导致推理卡顿，只有警告，长久不结束，但又不抛异常。

*** 
解决方案：

1. 方法1：
   - configuration_qwen.py中修改max_position_embeddings=32768
   - tokenizer_config.json中修改max_position_embeddings=32768
2. 方法2：
    - 修改文件：site-packages.vllm.config.py中的get_max_model_len(self)函数。
    - 同时开启server时候设置：--max-num-batched-tokens 8192

# 参考

[1] VLLM-百川ALiBi_NTK拓展后修改max_model_len，https://zhuanlan.zhihu.com/p/654743474