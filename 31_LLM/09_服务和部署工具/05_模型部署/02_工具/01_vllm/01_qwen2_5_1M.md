# 1. è¦ç‚¹é€Ÿè¯»

## 1.1 å®æµ‹è¯„ä»·

æŒ‰ç…§å‚è€ƒï¼šhttps://modelscope.cn/models/Qwen/Qwen2.5-14B-Instruct-1M

- å®˜ç½‘vllm
  - ä»…æ”¯æŒ262,144 tokens
- é­”æ”¹ç‰ˆvllm
  - ç›®å‰ä½¿ç”¨qwenè‡ªå·±é­”æ”¹çš„vllmä»£ç ï¼Œè¿˜æœªåˆå…¥vllmåˆ†æ”¯ï¼Œæ‰€ä»¥éœ€è¦è‡ªå·±ç¼–è¯‘vllmä»£ç ã€‚
  - æ”¯æŒ1,010,000 tokensè¾“å…¥å’Œ8192 tokensè¾“å‡º
  - å¦‚æœä¸é€‚ç”¨é­”æ”¹ç‰ˆæœ¬ï¼Œè¶…è¿‡262,144 tokensï¼Œæ€§èƒ½ä¼šè¡°å‡ï¼Œè€Œä¸”ä¸èƒ½ä½¿ç”¨sparse attention and length extrapolation 
  - sparse attention and length extrapolationå¯å®ç°3åˆ°7å€åŠ é€Ÿ 

å‘ç‚¹ï¼š
- vllmå‘½ä»¤è¡Œå¯åŠ¨ï¼Œå¤§æ¦‚ç‡ä¸‹æ— æ³•åœæ­¢è¿›ç¨‹ï¼Œkillä¹Ÿæ— æ³•æ€æ­»ï¼Œéœ€è¦é‡å¯æœºå™¨ï¼Œè¾ƒä¸ºéº»çƒ¦
  - ä½¿ç”¨pythonå°è£…openaiæ¥å£ï¼Œå†…éƒ¨è°ƒç”¨vllmåº“ï¼ŒæŒ‚æ­»çš„æ¦‚ç‡å¾ˆå° ï¼ˆğŸ¯ä»£ç è¯¦è§ï¼šhttps://t.zsxq.com/q42Js ï¼‰
- vllmå¯åŠ¨æ—¶ï¼Œgpu_memory_utilizationå‚æ•°è®¾å°äº›ï¼Œç”±äºdual-chunk-attnçš„å®ç°é‡Œé¢å¯¹äºé•¿æ–‡æœ¬çš„å¤„ç†ï¼Œä¼šå ç”¨æ›´å¤šçš„kv-cacheï¼Œ
   ä½†æ˜¯vllmåˆæ²¡æœ‰è®¡ç®—è¿™ä¸€å—ï¼Œæ‰€ä»¥å¦‚æœè®¾ç½®è¿‡å¤§ï¼Œä¸”è¾“å…¥åºåˆ—è¾ƒé•¿ï¼Œå¾ˆå‡ºç°çˆ†æ˜¾å­˜é—®é¢˜
- 1Mé•¿æ–‡æœ¬å¯¼è‡´æ¨ç†é€Ÿåº¦å¾ˆæ…¢ï¼Œå¯å¼€å¯enable-prefix-cachingï¼Œå¼€å¯åé€Ÿåº¦æ˜æ˜¾å¾ˆå¿«ï¼Œä½†æ˜¯ä¼šå¯¼è‡´å†…å­˜å ç”¨å¤§é‡å¢åŠ 
- kv-cache-dtypeè®¾ä¸ºfp8ä¼šæŠ¥é”™

å®æµ‹ï¼š
- å®æµ‹å¦‚æœ`gpu_memory_utilization`è®¾ä¸º0.98ï¼Œè¾“å…¥960kï¼Œåœ¨8*80Gæ˜¾å¡ä¸Šï¼Œä¼šç¬é—´çˆ†æ˜¾å­˜ã€‚è®¾ä¸º0.5ï¼Œè¾“å…¥960kï¼Œæ­£å¸¸è¿è¡Œã€‚
- ç”¨transformers+torchåº“å¯åŠ¨ï¼Œ8å¡80Gï¼Œè¾“å…¥960kï¼Œä¼šçˆ†æ˜¾å­˜ã€‚

## 1.2 èµ„æº

**è®ºæ–‡åŠæŠ€æœ¯æŠ¥å‘Š**
- Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens
  - https://qwenlm.github.io/blog/qwen2.5-1m/
- Qwen2.5-1M Technical Report
  - arXiv

**ä»£ç **
- Github (15.6k stars):https://github.com/QwenLM/Qwen2.5
åƒé—®ä½¿ç”¨æ–‡æ¡£ï¼šhttps://qwen.readthedocs.io/en/latest/deployment/vllm.html

# 2. é…ç½®è¦æ±‚

- transformers>=4.37.0, å¦åˆ™æŠ¥qwen2 key error
- CUDA Version: 12.1 or 12.3
- Python Version: >=3.9 and <=3.12

1M-tokené•¿VRAMè¦æ±‚:
- Qwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs).
- Qwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs).

# 3. æŒ‰ç…§

```bash
git clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git
cd vllm
pip install -e . -v
```

å¦‚æœgithubå­˜åœ¨è¿æ¥é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åœ¨æµè§ˆå™¨å…‹éš†åˆ†æ”¯ï¼šhttps://github.com/QwenLM/vllm/tree/dev/dual-chunk-attn

# 4. ä½¿ç”¨

````python
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-14B-Instruct-1M"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
````

## 4.1 æœåŠ¡ç«¯å¯åŠ¨
### 4.1.1 ç¦»çº¿ä½¿ç”¨

```python
from modelscope import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-14B-Instruct-1M")

# Pass the default decoding hyperparameters of Qwen2.5-14B-Instruct
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)

# Input the model name or path. See below for parameter explanation (after the example of openai-like server).
llm = LLM(model="Qwen/Qwen2.5-14B-Instruct-1M",
    tensor_parallel_size=4,
    max_model_len=1010000,
    enable_chunked_prefill=True,
    max_num_batched_tokens=131072,
    enforce_eager=True,
    # quantization="fp8", # Enabling FP8 quantization for model weights can reduce memory usage.
)

# Prepare your prompts
prompt = "Tell me something about large language models."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

### 4.1.2 åœ¨çº¿ä½¿ç”¨

```bash
vllm serve Qwen/Qwen2.5-14B-Instruct-1M \
  --tensor-parallel-size 4 \
  --max-model-len 1010000 \
  --enable-chunked-prefill --max-num-batched-tokens 131072 \
  --enforce-eager \
  --max-num-seqs 1

# --quantization fp8  # Enabling FP8 quantization for model weights can reduce memory usage.
```

å‚æ•°è¯´æ˜:

**--tensor-parallel-size**

è®¾ç½®ä¸ºä½ ä½¿ç”¨çš„ GPU æ•°é‡ã€‚å¯¹äº 7B æ¨¡å‹ï¼Œæœ€å¤§æ”¯æŒ 4 ä¸ª GPUï¼Œå¯¹äº 14B æ¨¡å‹ï¼Œæœ€å¤§æ”¯æŒ 8 ä¸ª GPUã€‚

**--max-model-len**

å®šä¹‰æœ€å¤§è¾“å…¥åºåˆ—é•¿åº¦ã€‚å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³é—®é¢˜ï¼Œå¯ä»¥å‡å°‘è¿™ä¸ªå€¼ã€‚

**--max-num-batched-tokens**

è®¾ç½®åˆ†å—é¢„å¡«å……ä¸­çš„æ‰¹å¤„ç†å¤§å°ã€‚è¾ƒå°çš„å€¼å¯ä»¥å‡å°‘æ¿€æ´»å†…å­˜çš„ä½¿ç”¨ï¼Œä½†å¯èƒ½ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚å»ºè®®å°†å…¶è®¾ç½®ä¸º 131072ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

**--max-num-seqs**

é™åˆ¶å¹¶å‘å¤„ç†çš„åºåˆ—æ•°ã€‚

# 5. å¸¸è§é—®é¢˜

**æ•…éšœæ’é™¤ï¼š**

1. é‡åˆ°é”™è¯¯ï¼šâ€œæ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆxxxxxï¼‰å¤§äºå¯ä»¥å­˜å‚¨åœ¨KVç¼“å­˜ä¸­çš„æœ€å¤§ä»¤ç‰Œæ•°é‡ã€‚â€

   KVç¼“å­˜åˆ†é…çš„æ˜¾å­˜ä¸è¶³ã€‚è€ƒè™‘å‡å°‘ `max_model_len` æˆ–å¢åŠ  `tensor_parallel_size`ã€‚å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥å‡å°‘ `max_num_batched_tokens`ï¼Œå°½ç®¡è¿™å¯èƒ½ä¼šæ˜¾è‘—é™ä½æ¨ç†é€Ÿåº¦ã€‚

2. é‡åˆ°é”™è¯¯ï¼šâ€œtorch.OutOfMemoryError: CUDA å†…å­˜ä¸è¶³ã€‚â€

   ç”¨äºæ¿€æ´»æƒé‡çš„æ˜¾å­˜ä¸è¶³ã€‚ä½ å¯ä»¥å°è¯•å°† `gpu_memory_utilization` è®¾ç½®ä¸º 0.85 æˆ–æ›´ä½ï¼Œä½†è¯·æ³¨æ„ï¼Œè¿™å¯èƒ½ä¼šå‡å°‘åˆ†é…ç»™ KV ç¼“å­˜çš„æ˜¾å­˜ã€‚

3. é‡åˆ°é”™è¯¯ï¼šâ€œè¾“å…¥æç¤ºï¼ˆxxxxx ä¸ªä»¤ç‰Œï¼‰+å‰ç»æ’æ§½ï¼ˆ0ï¼‰å¤ªé•¿ï¼Œè¶…å‡ºäº†å—ç®¡ç†å™¨çš„å®¹é‡ã€‚â€

   è¾“å…¥è¿‡é•¿ã€‚è€ƒè™‘ä½¿ç”¨æ›´çŸ­çš„åºåˆ—æˆ–å¢åŠ  `max_model_len`ã€‚

**æ³¨æ„**

ç”±äºdual-chunk-attnçš„å®ç°é‡Œé¢å¯¹äºé•¿æ–‡æœ¬çš„å¤„ç†ï¼Œä¼šå ç”¨æ›´å¤šçš„kv-cacheï¼Œä½†æ˜¯vllmåˆæ²¡æœ‰è®¡ç®—è¿™ä¸€å—(ä¼°è®¡æ˜¯åƒé—®å›¢é˜Ÿè¿˜æœªä¼˜åŒ–è¿™ä¸€å—ï¼Œæ‰€ä»¥è¿™ä¸ªåˆ†æ”¯ä¹Ÿæœªåˆå…¥vllmä¸»åˆ†æ”¯)ï¼Œ
æ‰€ä»¥å¦‚æœè®¾ç½®`gpu_memory_utilization`å‚æ•°è¿‡å¤§ï¼Œä¸”è¾“å…¥åºåˆ—è¾ƒé•¿ï¼Œå¯èƒ½å‡ºç°çˆ†æ˜¾å­˜é—®é¢˜ã€‚
