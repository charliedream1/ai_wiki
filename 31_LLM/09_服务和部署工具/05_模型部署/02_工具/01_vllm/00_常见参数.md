在使用 vLLM 进行大语言模型（LLM）推理时，启动参数的配置对显存占用和推理效率有显著影响。以下是关键参数及其优化建议：

1. gpu_memory_utilization：该参数控制每个 GPU 的显存预分配比例，默认值为 0.9。适当调整此值可以平衡显存占用和推理性能。例如，将 gpu_memory_utilization 设置为 0.15，可以将显存占用控制在 21GB，从而避免显存过度占用。

2. max_model_len：该参数定义模型的最大上下文长度，直接影响 KV 缓存的大小和显存需求。适当减少 max_model_len 的值，可以降低显存占用。例如，将 max_model_len 设置为 600，可以在单个 NVIDIA L4 GPU 上运行默认上下文长度较长的模型。

3. tensor_parallel_size：该参数指定模型权重在多少个 GPU 之间进行切分。增加 tensor_parallel_size 可以减少每个 GPU 的显存负担，但可能会引入额外的通信开销。建议根据硬件配置和模型大小进行调整。

4. max_num_seqs 和 max_num_batched_tokens：这两个参数控制每个批次中的并发请求数量和令牌数量。适当减少这些值，可以降低对 KV 缓存空间的需求，从而减少显存占用。例如，将 max_num_batched_tokens 设置为 512，可以在初始基准测试中获得较好的令牌间延迟（ITL）。 dtype：设置为 “fp16” 可以启用混合精度推理，减少显存占用。

5. offload_weights和 offload_activations：启用这些参数可以将模型权重和激活值部分卸载到 CPU，从而减少 GPU 显存占用

# 参考

[1] 大模型推理引擎对比SGLang vs vLLM，最佳选择？https://mp.weixin.qq.com/s/Ckhf7KAXvWGe1ZW9WnD0hg