具体参考帮助文档，使用GPU并行、内存优化、量化等技术。RadixAttention 是一种用于提升大型语言模型（LLM）效率的技术，它通过前缀缓存、跳转约束解码、无需额外开销的 CPU 调度器、连续批处理、令牌注意力（分页注意力）、张量并行性等技术优化模型的推理过程。此外，它还使用 FlashInfer 内核、分块预填充和量化技术（包括 FP8/4/AWQ/GPTQ）来进一步加速模型的响应速度和降低计算成本

1.  多 GPU 并行参数
    - tp: 设置张量并行的 GPU 数量。例如 - tp 2 表示使用 2 个 GPU 进行张量并行。
    - dp : 设置数据并行的 GPU 数量。例如 - dp 2 表示使用 2 个 GPU 进行并行。

2. 内存优化参数

   - mem-fraction-static: 设置 KV 缓存池的大小比例，用于优化内存使用。例如 - mem-fraction-static 0. 8 表示将 80% 的内存用于 KV 缓存池。

3. 量化加速参数

   - enable-torch-compile: 启用 PyTorch 的编译优化，加速模型推理。

   - torchao-config: 指定 torch 量化配置文件，用于模型量化加速。

   - quantization: 设置权重量化类型。例如 - quantization fp8 表示使用 fp8 权重量化。

   - kv-cache-dtype: 设置 KV 缓存的量化数据类型。例如 - kv-cache-dtype fp8_e5m2 表示使用 fp8_e5m2 类型的 KV 缓存量化。

4. 分布式参数

   - nnode: 设置分布式训练的总节点数。例如 - nnodes 2 表示使用 2 个节点进行分布式推理。

   - node-rank: 设置当前节点的排名，用于分布式训练中的节点标识。例如 - node-rank 0 表示当前节点是第一个节点。

5. 其他参数

   - use-ray: 启用 Ray 分布式框架进行训练。

   - use-deeps: 启用 DeepSpeed 加速库进行训练。

   - use-lora: 启用 LoRA 微调技术。

# 参考

[1] 大模型推理引擎对比SGLang vs vLLM，最佳选择？https://mp. weixin. qq. com/s/Ckhf7KAXvWGe1ZW9WnD0hg
