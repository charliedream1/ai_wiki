# 1. 要点速读

## 1.1 实测评价

按照参考：https://modelscope.cn/models/Qwen/Qwen2.5-14B-Instruct-1M

- 官网vllm
  - 仅支持262,144 tokens
- 魔改版vllm
  - 目前使用qwen自己魔改的vllm代码，还未合入vllm分支，所以需要自己编译vllm代码。
  - 支持1,010,000 tokens输入和8192 tokens输出
  - 如果不适用魔改版本，超过262,144 tokens，性能会衰减，而且不能使用sparse attention and length extrapolation 
  - sparse attention and length extrapolation可实现3到7倍加速 

坑点：
- vllm命令行启动，大概率下无法停止进程，kill也无法杀死，需要重启机器，较为麻烦
  - 使用python封装openai接口，内部调用vllm库，挂死的概率很小 （🎯代码详见：https://t.zsxq.com/q42Js ）
- vllm启动时，gpu_memory_utilization参数设小些，由于dual-chunk-attn的实现里面对于长文本的处理，会占用更多的kv-cache，
   但是vllm又没有计算这一块，所以如果设置过大，且输入序列较长，很出现爆显存问题
- 1M长文本导致推理速度很慢，可开启enable-prefix-caching，开启后速度明显很快，但是会导致内存占用大量增加
- kv-cache-dtype设为fp8会报错

实测：
- 实测如果`gpu_memory_utilization`设为0.98，输入960k，在8*80G显卡上，会瞬间爆显存。设为0.5，输入960k，正常运行。
- 用transformers+torch库启动，8卡80G，输入960k，会爆显存。

## 1.2 资源

**论文及技术报告**
- Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens
  - https://qwenlm.github.io/blog/qwen2.5-1m/
- Qwen2.5-1M Technical Report
  - arXiv

**代码**
- Github (15.6k stars):https://github.com/QwenLM/Qwen2.5
千问使用文档：https://qwen.readthedocs.io/en/latest/deployment/vllm.html

# 2. 配置要求

- transformers>=4.37.0, 否则报qwen2 key error
- CUDA Version: 12.1 or 12.3
- Python Version: >=3.9 and <=3.12

1M-token长VRAM要求:
- Qwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs).
- Qwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs).

# 3. 按照

```bash
git clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git
cd vllm
pip install -e . -v
```

如果github存在连接问题，也可以在浏览器克隆分支：https://github.com/QwenLM/vllm/tree/dev/dual-chunk-attn

# 4. 使用

````python
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-14B-Instruct-1M"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
````

## 4.1 服务端启动
### 4.1.1 离线使用

```python
from modelscope import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-14B-Instruct-1M")

# Pass the default decoding hyperparameters of Qwen2.5-14B-Instruct
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)

# Input the model name or path. See below for parameter explanation (after the example of openai-like server).
llm = LLM(model="Qwen/Qwen2.5-14B-Instruct-1M",
    tensor_parallel_size=4,
    max_model_len=1010000,
    enable_chunked_prefill=True,
    max_num_batched_tokens=131072,
    enforce_eager=True,
    # quantization="fp8", # Enabling FP8 quantization for model weights can reduce memory usage.
)

# Prepare your prompts
prompt = "Tell me something about large language models."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

### 4.1.2 在线使用

```bash
vllm serve Qwen/Qwen2.5-14B-Instruct-1M \
  --tensor-parallel-size 4 \
  --max-model-len 1010000 \
  --enable-chunked-prefill --max-num-batched-tokens 131072 \
  --enforce-eager \
  --max-num-seqs 1

# --quantization fp8  # Enabling FP8 quantization for model weights can reduce memory usage.
```

参数说明:

**--tensor-parallel-size**

设置为你使用的 GPU 数量。对于 7B 模型，最大支持 4 个 GPU，对于 14B 模型，最大支持 8 个 GPU。

**--max-model-len**

定义最大输入序列长度。如果遇到内存不足问题，可以减少这个值。

**--max-num-batched-tokens**

设置分块预填充中的批处理大小。较小的值可以减少激活内存的使用，但可能会降低推理速度。建议将其设置为 131072，以获得最佳性能。

**--max-num-seqs**

限制并发处理的序列数。

# 5. 常见问题

**故障排除：**

1. 遇到错误：“模型的最大序列长度（xxxxx）大于可以存储在KV缓存中的最大令牌数量。”

   KV缓存分配的显存不足。考虑减少 `max_model_len` 或增加 `tensor_parallel_size`。另外，你也可以减少 `max_num_batched_tokens`，尽管这可能会显著降低推理速度。

2. 遇到错误：“torch.OutOfMemoryError: CUDA 内存不足。”

   用于激活权重的显存不足。你可以尝试将 `gpu_memory_utilization` 设置为 0.85 或更低，但请注意，这可能会减少分配给 KV 缓存的显存。

3. 遇到错误：“输入提示（xxxxx 个令牌）+前瞻插槽（0）太长，超出了块管理器的容量。”

   输入过长。考虑使用更短的序列或增加 `max_model_len`。

**注意**

由于dual-chunk-attn的实现里面对于长文本的处理，会占用更多的kv-cache，但是vllm又没有计算这一块(估计是千问团队还未优化这一块，所以这个分支也未合入vllm主分支)，
所以如果设置`gpu_memory_utilization`参数过大，且输入序列较长，可能出现爆显存问题。
