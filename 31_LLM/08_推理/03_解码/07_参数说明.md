大型语言模型（LLMs）的每一次输出都蕴含着强大的能力，许多用户会发现，模型的生成结果有时显得随机或不稳定。这并非模型不可控，而是因为我们尚未充分利用那些影响其输出的“幕后推手”——生成控制参数。

这些参数是LLM从内部概率分布中选择词元（Token）时遵循的核心指令，它们决定了模型的：

风格： 倾向于创新还是严谨？
结构： 长度如何限制？何时自动停止？
倾向： 是否鼓励话题多样性或避免重复？
掌握这些关键参数，是实现LLM从“能够生成”到“精准控制”的关键飞跃。接下来将深入解析影响LLM输出的七个核心控制参数，为您提供定制和优化模型表现的实用指南。

控制随机性与多样性的三大策略
这组参数直接影响模型在选择下一个词元（Token）时的不确定性，是控制模型创意性与确定性的核心。

1. Temperature（温度）
参数
描述
最佳场景
低温度
 (e.g., 0.2)
使模型的输出更确定性 (Deterministic) 和聚焦。模型几乎总是选择概率最高的词元。
事实性任务
（如摘要、代码生成、直接问答）。
高温度
 (e.g., 1.0)
使输出更随机和有创意。模型更有可能选择概率较低的词元，产生新颖且多样的文本。
创意写作
、头脑风暴、开放式对话。
原理： 温度通过缩放词元（Logits）的概率分布来控制采样时的平滑度。温度越低，概率分布越“尖锐”，高概率词元被选中的可能性越高。

2. Top-p (核心采样)
核心机制：Top-p（也称核采样）选择一个动态的词元子集。模型会考虑概率累积和达到指定阈值  的最小词元集合。

示例： 若 ，模型仅从累积概率占前  的词元中进行选择。

优势： Top-p 在保证一定随机性的同时，能有效避免选择那些概率极低、可能导致荒谬输出的词元，因此常被推荐作为温度参数的替代或辅助手段，提供良好的平衡性。

3. Top-k (固定数量采样)
核心机制：Top-k 将模型的选择范围限制在概率最高的  个词元内。

示例： 若 ，模型只会在概率最高的 50 个词元中进行采样，不论这 50 个词元的累积概率是多少。

优势： Top-k 是一种更直接的限制方法，可以有效防止模型选择到那些异常低概率的词元，使输出在保持连贯性方面优于单纯的高温度采样。

控制长度与结构的参数
这些参数主要用于对模型生成过程的边界和终止条件进行硬性约束。

4. Max Tokens / Max New Tokens（最大词元数）
核心机制： 设置模型在单次API调用中可以生成的最大词元数量的硬性上限。

作用： 严格控制响应长度，有效管理计算成本，并防止模型陷入无限循环或无意义的冗长输出（即“跑飞”）。

5. Stop Sequences（停止序列）
核心机制： 用户定义的一个或一组字符字符串。一旦模型生成了这些序列中的任何一个，输出将立即终止。

作用： 极其关键的结构化工具，用于控制输出格式（例如 JSON 对象的结束符）、创建问答或对话中的回合转换，以及模拟代码块或特定文档的结尾。

控制重复性与主题切换的惩罚机制
惩罚机制旨在调整词元的选择概率，以鼓励多样性和新颖性。

6. Frequency Penalty（频率惩罚）
核心机制： 根据词元在已生成文本中出现的次数来惩罚（降低其被选中的概率）。

正值（如 ）：鼓励语言多样性，降低模型重复使用相同词语的倾向。零值（）：无惩罚。

7. Presence Penalty（存在惩罚）
核心机制： 只要词元在已生成文本中出现过至少一次，就对其应用一次性的惩罚。

正值（如 ）：鼓励引入新概念和新主题。因为它惩罚的是词元是否出现过，而非出现的频率，能有效避免模型被限制在一个想法或主题上，推动其进行主题切换。

总结
生成参数是LLM的“调音台”。在实际应用中，通常需要组合使用这些参数以达到最佳效果：

对于严谨的任务：使用低 Temperature 或中高 Top-p（如 0.8-0.95），并设置适当的 Max Tokens 和 Stop Sequences。
对于创意任务：使用较高 Temperature（如 0.8-1.0），同时配合 Frequency Penalty 和 Presence Penalty 来鼓励新颖和多样的输出。
理解并熟练调整这些控制项，是实现LLM精准控制和高性能输出的必经之路。

# 参考

[1] LLM高手必备！一文吃透7大核心参数，实现大模型输出的精准控制！https://mp.weixin.qq.com/s/0imVXeOYpS8MNrJAyyBbEQ