# 1. 资源

- 快速入门：https://ollama.fan/getting-started/
- 下载安装文件：https://ollama.com/download/linux
- 支持的模型：https://ollama.com/library
- 常见问题：https://ollama.fan/resources/faq/

# 1. 简介

ollama是llama.cpp的一个封装，使其更加简介，可以在cpu/gpu混合推理使用。

# 2. 安装

下载安装文件：https://ollama.com/download/linux

linux下安装：

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

通过该命令，可完成自动安装

# 3. 使用
## 3.1 查看参数配置

```bash
ollama --help
```

注意在ollama下的选项，还可以继续查看下一级的参数配置，比如，如下查看run的参数配置
    
```bash
ollama run --help
```

## 3.2 修改环境变量

注：在3.3中会介绍环境变量随serve启动配置

在如下常见问题链接中，也提供了修改环境变量的方法，比如linux下使用systemctl，但我的电脑怎么也用不了

- 常见问题：https://ollama.fan/resources/faq/

可以使用export导入环境变量的方法

如下，查看serve下的可配置环境变量

```bash
ollama serve --help
```

比如需要修改ip及端口号，可以使用如下命令

```bash
export OLLAMA_HOST=0.0.0.0:80
```

注意，
- 后续的模型命令，也需要使用相同的环境变量，否则会提示找不到服务
- 使用0.0.0.0，否则外部无法访问
- 使用127.0.0.1，仅能在本地访问

OLLAMA_KEEP_ALIVE          The duration that models stay loaded in memory (default "5m")

注意：如果不设置，模型默认仅保存5分钟，之后会被释放


## 3.3 启动ollama服务

```bash
ollama serve
```

## 3.4 使用自动下载模型
查看支持的模型，如果没有提供本地模型，会自动下载

```bash
ollama run llama3
```

如果配置环境变量

```bash
OLLAMA_DEBUG=1 OLLAMA_HOST=0.0.0.0:9020 OLLAMA_KEEP_ALIVE=72h OLLAMA_NUM_PARALLEL=8 nohup ollama serve &> server.log &
```

以上环境变量：
1. OLLAMA_DEBUG=1 打印debug，正式运⾏需要关闭
2. OLLAMA_HOST=0.0.0.0:9020 修改监听地址
3. OLLAMA_KEEP_ALIVE=72h 保持三天，避免频繁卸载
4. OLLAMA_NUM_PARALLEL=8 批量8槽位，需要进⾏调优测试

## 3.5 使用本地模型
### 3.5.1 定制模型

从 GGUF 导入

Ollama 支持在 Modelfile 中导入 GGUF 模型：

1. 创建一个名为 Modelfile 的文件，其中包含一个指向你想导入模型的本地文件路径的 FROM 指令。

   如下，设置模型长度为8192，如果模型并发设为，2，则长度设为用8192 * 2

    ```bash
    FROM ./vicuna-33b.Q4_0.gguf
    PARAMETER num_ctx 8192
    ```
      
   num_ctx 按每个并发10240 token计算，8个并发共需81920 

   注意：并发数在启动时设置，如果不设置，会自动调整，导致最大长度不好计算，所以建议设为固定的。

2. 在 Ollama 中创建模型

    ```bash
    ollama run llama3 --model Modelfile
    ```

3. 运行模型

    ```bash
    ollama run llama3 "What is your favourite condiment?"
    ```
   
### 3.5.2 从pytorch或safetensor导入

1. clone the ollama/ollama repo

   ```bash
   git clone git@github.com:ollama/ollama.git ollama
   cd ollama
   ```

2. fetch its llama.cpp submodule:

   ```bash
   git submodule init
   git submodule update llm/llama.cpp
   ```

3.  install the Python dependencies
   ```bash
   python3 -m venv llm/llama.cpp/.venv
   source llm/llama.cpp/.venv/bin/activate
   pip install -r llm/llama.cpp/requirements.txt
   ```

4. build the quantize tool:
   ```bash
   make -C llm/llama.cpp quantize
   ```

5. Clone the HuggingFace repository (optional)
   ```bash
   git lfs install
   git clone https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 model
   ```

6. Convert the model
   
   Note: some model architectures require using specific convert scripts. For example, Qwen models require running convert-hf-to-gguf.py instead of convert.py

   ```bash
   python llm/llama.cpp/convert.py ./model --outtype f16 --outfile converted.bin
   ```

7. Quantize the model
    ```bash
    llm/llama.cpp/quantize converted.bin quantized.bin q4_0
    ```
   
Then follow previous steps of using GGUF models.

## 3.6 运行参数

如何指定上下文窗口大小？

默认情况下，Ollama 使用 2048 个令牌的上下文窗口大小。

在使用 ollama run 时，使用 /set parameter 来更改：

```bash
/set parameter num_ctx 4096
```

在使用 API 时，指定 num_ctx 参数：

```bash
curl http://localhost:11434/api/generate -d '{

  "model": "llama2",

  "prompt": "Why is the sky blue?",

  "options": {

    "num_ctx": 4096

  }

}'
```

## 3.7 其它常用命令

```bash
# 查看在跑的模型
ollama ps

# 帮助
ollama -h
```

# 4. 使用及测试

generate接⼝测试

```bash
time curl http://localhost:9020/api/generate -d '{
 "model": "deepseek-16b:0.1.1",
 "stream": false,
 "prompt":"你好"
}'
```

openai兼容接⼝测试

```bash
time curl http://localhost:9020/v1/chat/completions \
 -H "Content-Type: application/json" \
 -d "{
 \"model\": \"deepseek-16b:0.1.1\",
 \"messages\": [
 {
 \"role\": \"user\",
 \"content\": \"你好\"
 },
 {
 \"role\": \"assistant\",
 \"content\": \"你好呀\"
 },
 {
 \"role\": \"user\",
 \"content\": \"帮我写首诗\"
 }
 ]
 }"

```
