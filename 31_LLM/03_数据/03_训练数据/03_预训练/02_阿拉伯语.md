200GB+高质量预训练文本，开源世界最大阿语预训练数据集
对预训练语言模型来说，构建高质量的大规模文本数据是保证模型训练效果的关键一环。经过多方深度合作与共同努力，以及对互联网现有阿拉伯语文本数据的采集、整理和清洗，并基于支撑WuDaoCorpora的自研网页文本深度清洗工具，研究团队针对阿语进行了高度适配和优化，并最终获得了超过200GB的高质量预训练语料ArabicText。

ArabicText成为当前开源世界数据量最大的阿拉伯语预训练数据集。相较于现有开源阿语文本数据集，ArabicText不仅体量全球最大，且新闻、资讯、百科等文字与知识富集类数据占比超过65%，有利于模型从数据中学习到更多的经验知识。
ArabicText数据集链接：
https://data.baai.ac.cn/details/ArabicText-2022