# 1. 资源

📂 开源链接：
🔗 https://github.com/THUDM/AgentRL
📄 论文原文：
🔗 https://arxiv.org/abs/2510.04206

# 2. 原理

📌 一句话总结：
本工作提出 AgentRL，一个面向多轮、多任务智能体强化学习（Agentic RL）的统一训练框架，通过异步管线、跨策略采样与任务归一化等创新设计，显著提升大模型智能体的泛化性与可扩展性。
🔍 背景问题：
当前基于强化学习的智能体（LLM Agents）仍面临两大核心瓶颈：
1️⃣ 单任务与单轮限制 —— 现有 RLHF 或 RLVR 方法多停留在单步决策层面，无法应对多轮环境交互与复杂任务调度；
2️⃣ 缺乏统一的多任务基础设施 —— 不同任务环境接口异构，导致模型无法在多任务场景下稳定训练或高效迁移。
💡 方法简介：
AgentRL 从基础架构与算法两端入手，系统化重构了智能体的强化学习流程：
⚙️ 异步多轮管线（Asynchronous Pipeline）：通过分离 rollout 与训练阶段，实现无阻塞的并行调度，GPU 利用率显著提升。
🧩 跨策略采样（Cross-Policy Sampling）：在同一轨迹中融合多模型决策，提高探索多样性，缓解模型坍塌问题。
🧠 任务优势归一化（Task Advantage Normalization）：针对任务异质性（长度、难度差异）引入标准化优势估计，稳定多任务训练过程。
🧱 统一环境接口与容器化部署：采用函数调用式 API + 控制器架构，实现成千上万个异构环境的并行管理与生命周期调度。
📊 实验结果：
在 ALFWorld、DB、KG、OS、Webshop 五个多轮环境上，AgentRL 的平均成功率达 70.4%，超越 GPT-5、Claude-Sonnet-4、DeepSeek-R1 等顶级模型；
单模型多任务训练 能匹配五个单任务专家模型的最优结果，并保持跨任务一致性；
在 BFCL-v3 的泛化测试中，AgentRL 进一步提升多轮任务表现，验证了多任务 RL 的迁移潜力；
消融实验表明：移除跨策略采样或任务归一化均导致性能下降 5–8%，证明两项机制对稳定性与探索性均至关重要。

✨ 一句话点评：
从单任务强化学习迈向“多轮多任务智能体”的关键一步，AgentRL 让 LLM 真正具备了“通用行动智能”的训练基础。

# 参考

[1] 清华提出 AgentRL：让大语言模型真正成为“多任务智能体”的强化学习框架, https://mp.weixin.qq.com/s/z-4ELy3GSZtL19hO5Dfx5A?poc_token=HGSGDGmj7eIjUoRL94AF-M_CwYQ7tOzpasotTloa